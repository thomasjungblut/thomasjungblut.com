{"componentChunkName":"component---src-templates-blog-post-js","path":"/blogger/xgboost-validation-and-early-stopping/","result":{"data":{"site":{"siteMetadata":{"title":"Coding with Thomas"}},"markdownRemark":{"id":"542cb632-6a73-5f6c-97fd-9da2eaacba19","excerpt":"Hey people, While using XGBoost in Rfor some Kaggle competitions I always come to a stage where I want to do early stopping of the training based on a held-out…","html":"<p>Hey people,</p>\n<p>While using XGBoost in Rfor some Kaggle competitions I always come to a stage where I want to do early stopping of the training based on a held-out validation set.</p>\n<p>There are very little code snippets out there to actually do it in R, so I wanted to share my quite generic code here on the blog.</p>\n<p>Let’s say you have a training set in some csv and you want to split that into a 9:1 training:validation set. Here’s the naive (not stratified way) of doing it:</p>\n<div class=\"gatsby-highlight\" data-language=\"r\"><pre class=\"language-r\"><code class=\"language-r\">train <span class=\"token operator\">&lt;-</span> read.csv<span class=\"token punctuation\">(</span><span class=\"token string\">\"train.csv\"</span><span class=\"token punctuation\">)</span>  \nbound <span class=\"token operator\">&lt;-</span> floor<span class=\"token punctuation\">(</span>nrow<span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">)</span> <span class=\"token operator\">*</span> <span class=\"token number\">0.9</span><span class=\"token punctuation\">)</span>           \ntrain <span class=\"token operator\">&lt;-</span> train<span class=\"token punctuation\">[</span>sample<span class=\"token punctuation\">(</span>nrow<span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">]</span>         \ndf.train <span class=\"token operator\">&lt;-</span> train<span class=\"token punctuation\">[</span><span class=\"token number\">1</span><span class=\"token operator\">:</span>bound<span class=\"token punctuation\">,</span> <span class=\"token punctuation\">]</span>                  \ndf.validation <span class=\"token operator\">&lt;-</span> train<span class=\"token punctuation\">[</span><span class=\"token punctuation\">(</span>bound<span class=\"token operator\">+</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token operator\">:</span>nrow<span class=\"token punctuation\">(</span>train<span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> <span class=\"token punctuation\">]</span></code></pre></div>\n<p>Now before feeding it back into XGBoost, we need to create a xgb.DMatrix and remove the targets to not spoil the classifier. This can be done via this code:</p>\n<div class=\"gatsby-highlight\" data-language=\"r\"><pre class=\"language-r\"><code class=\"language-r\">train.y <span class=\"token operator\">&lt;-</span> df.train<span class=\"token operator\">$</span>TARGET  \nvalidation.y <span class=\"token operator\">&lt;-</span> df.validation<span class=\"token operator\">$</span>TARGET  \n  \ndtrain <span class=\"token operator\">&lt;-</span> xgb.DMatrix<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span>df.train<span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span>train.y<span class=\"token punctuation\">)</span>  \ndvalidation <span class=\"token operator\">&lt;-</span> xgb.DMatrix<span class=\"token punctuation\">(</span>data<span class=\"token operator\">=</span>df.validation<span class=\"token punctuation\">,</span> label<span class=\"token operator\">=</span>validation.y<span class=\"token punctuation\">)</span></code></pre></div>\n<p>So now we can go and setup a watchlist and actually start the training. Here’s some simple sample code to get you started:</p>\n<div class=\"gatsby-highlight\" data-language=\"r\"><pre class=\"language-r\"><code class=\"language-r\">watchlist <span class=\"token operator\">&lt;-</span> list<span class=\"token punctuation\">(</span>validation<span class=\"token operator\">=</span>dvalidation<span class=\"token punctuation\">,</span> train<span class=\"token operator\">=</span>dtrain<span class=\"token punctuation\">)</span>  \n  \nparam <span class=\"token operator\">&lt;-</span> list<span class=\"token punctuation\">(</span>  \n   objective <span class=\"token operator\">=</span> <span class=\"token string\">\"binary:logistic\"</span><span class=\"token punctuation\">,</span>  \n   eta <span class=\"token operator\">=</span> <span class=\"token number\">0.3</span><span class=\"token punctuation\">,</span>  \n   max_depth <span class=\"token operator\">=</span> <span class=\"token number\">8</span>                  \n<span class=\"token punctuation\">)</span>  \n  \nclf <span class=\"token operator\">&lt;-</span> xgb.train<span class=\"token punctuation\">(</span>     \n   params <span class=\"token operator\">=</span> param<span class=\"token punctuation\">,</span>   \n   data <span class=\"token operator\">=</span> dtrain<span class=\"token punctuation\">,</span>   \n   nrounds <span class=\"token operator\">=</span> <span class=\"token number\">500</span><span class=\"token punctuation\">,</span>   \n   watchlist <span class=\"token operator\">=</span> watchlist<span class=\"token punctuation\">,</span>  \n   maximize <span class=\"token operator\">=</span> <span class=\"token boolean\">FALSE</span><span class=\"token punctuation\">,</span>  \n   early.stop.round <span class=\"token operator\">=</span> <span class=\"token number\">20</span>  \n<span class=\"token punctuation\">)</span></code></pre></div>\n<p>Here we setup a watchlist with the validation set in the first dimension of the list and the trainingset in the latter. The reason that you need to put the validation set first is that the early stopping only works on one metric - where we should obviously choose the validation set.</p>\n<p>The rest is straightforward setup of the xgb tree itself. Keep in mind that when you use early stopping, you also need to supply whether or not to maximize the chosen objective function- otherwise you might find yourself stopping very fast!</p>\n<p>Here’s the full snippet as a <a href=\"https://gist.github.com/thomasjungblut/e60217c5b7609e4dfef3\">gist</a></p>\n<p>Thanks for reading,<br>\nThomas</p>","frontmatter":{"title":"XGBoost Validation and Early Stopping in R","date":"18th March 2016","description":null},"tableOfContents":"","timeToRead":1},"previous":{"fields":{"slug":"/blogger/minhashing-for-beginners/"},"frontmatter":{"title":"MinHashing for Beginners"}},"next":{"fields":{"slug":"/blogger/xgboost-bayesian-hyperparameter-tuning/"},"frontmatter":{"title":"XGBoost bayesian hyperparameter tuning with bayes_opt in Python"}}},"pageContext":{"id":"542cb632-6a73-5f6c-97fd-9da2eaacba19","previousPostId":"20c92f62-8d52-566b-8cff-74a7b830a248","nextPostId":"b4dfe3e3-3857-5d8d-9e32-6a82cacebbf7"}},"staticQueryHashes":["2270107033","2841359383"],"slicesMap":{}}
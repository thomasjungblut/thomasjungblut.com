{"componentChunkName":"component---src-templates-blog-post-js","path":"/blogger/graph-exploration-with-hadoop-mapreduce/","result":{"data":{"site":{"siteMetadata":{"title":"Coding with Thomas"}},"markdownRemark":{"id":"b0f28283-b8f1-5d28-976f-653b973f4bb4","excerpt":"Hi all, sometimes you will have data where you don’t know how elements of these data are connected. This is a common usecase for graphs, this is because they…","html":"<p>Hi all,<br>\nsometimes you will have data where you don’t know how elements of these data are connected. This is a common usecase for graphs, this is because they are really abstract.<br>\nSo if you don’t know how your data is looking like, or if you know how it looks like and you just want to determine various graph components, this post is a good chance for you to get the “MapReduce-way” of graph exploration. As already mentioned in my previous post, I ranted about message passing through DFS and how much overhead it is in comparison to BSP.<br>\nJust let this be a competition of Apache Hama BSP and Apache Hadoop MapReduce. Both sharing the HDFS as a distributed FileSystem.<br>\nLooking at the title you know that this post is about the MapReduce implementation, I write a BSP implementation later and compare this with this MapReduce implementation.<br>\nLet’s introduce the prequisites now.</p>\n<p><strong>Prequisites</strong><br>\nWe have a graph in a format of an adjacency list looking like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">0  \n1    4    7  \n2    3    8  \n3    5  \n4    1  \n5    6  \n6  \n7      \n8    3  \n9    0  \n</code></pre></div>\n<p>So the first entry on the left side is always the vertex, therefore all vertices are listed on the leftmost side. Each vertex is described by a number: its id.<br>\nSeparated by tabs are the vertex ids that are adjacent to the vertex on the leftmost side.</p>\n<p>This is quite abstract so let’s take a look at this pretty picture:</p>\n<p><a href=\"http://4.bp.blogspot.com/-Dtk6PB2ynwo/TZ9ESh9LN7I/AAAAAAAAAQM/BSBA0DDUCBs/s1600/Unbenannt.PNG\"><img src=\"http://4.bp.blogspot.com/-Dtk6PB2ynwo/TZ9ESh9LN7I/AAAAAAAAAQM/BSBA0DDUCBs/s320/Unbenannt.PNG\" alt=\"graph with multiple components\"></a></p>\n<p>This is how this graph looks like. As you can see there are three components: [1,4,7];[2,3,5,6,8];[0,9].<br>\nIn some datasets you want to classify each component to a common key that is unique. In this case it is the most common solution to just let a component be classified by its lowest id. E.G the component [1,4,7] has the lowest id 1. It is the classifier for this component.</p>\n<p><strong>How do we deal with this in MapReduce?</strong><br>\nFirst of all I recommend you to read into this <a href=\"http://www.umiacs.umd.edu/%7Ejimmylin/publications/Lin_Schatz_MLG2010.pdf\">paper</a>. It describes a technique named “message passing”.<br>\nSimple: The idea behind this is, that you let the vertices pass messages if a new local minima has been found. Afterwards you are just merging the messages with the real vertices and apply updates on the vertices that had a higher minimum.</p>\n<p>So our first task is to write the value class that is representing a vertex AND a message at the same time.</p>\n<div class=\"gatsby-highlight\" data-language=\"java\"><pre class=\"language-java\"><code class=\"language-java\"><span class=\"token keyword\">public</span> <span class=\"token keyword\">class</span> <span class=\"token class-name\">VertexWritable</span> <span class=\"token keyword\">implements</span> <span class=\"token class-name\">Writable</span><span class=\"token punctuation\">,</span> <span class=\"token class-name\">Cloneable</span> <span class=\"token punctuation\">{</span>  \n  \n <span class=\"token class-name\">LongWritable</span> minimalVertexId<span class=\"token punctuation\">;</span>  \n <span class=\"token class-name\">TreeSet</span><span class=\"token generics\"><span class=\"token punctuation\">&lt;</span><span class=\"token class-name\">LongWritable</span><span class=\"token punctuation\">></span></span> pointsTo<span class=\"token punctuation\">;</span>  \n <span class=\"token keyword\">boolean</span> activated<span class=\"token punctuation\">;</span>  \n          \n<span class=\"token keyword\">public</span> <span class=\"token keyword\">boolean</span> <span class=\"token function\">isMessage</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>  \n  <span class=\"token keyword\">if</span> <span class=\"token punctuation\">(</span>pointsTo <span class=\"token operator\">==</span> <span class=\"token keyword\">null</span><span class=\"token punctuation\">)</span>  \n   <span class=\"token keyword\">return</span> <span class=\"token boolean\">true</span><span class=\"token punctuation\">;</span>  \n  <span class=\"token keyword\">else</span>  \n   <span class=\"token keyword\">return</span> <span class=\"token boolean\">false</span><span class=\"token punctuation\">;</span>  \n <span class=\"token punctuation\">}</span>  \n  \n<span class=\"token punctuation\">}</span>  \n</code></pre></div>\n<p>And the typical read/write stuff coming with Writable.<br>\nSo let me explain to you, we have this class representing the Vertex: it has a pointsTo tree that will maintain the adjacent vertex ids and the currently minimalVertexId. And there is also a boolean field that is called “activated”.<br>\nThere is also a method that determines whether this is representing a message or a vertex.</p>\n<p>The whole thing is just working like this:</p>\n<ol>\n<li>Import the vertices from the adjacency list to the ID mapped to Vertex form.</li>\n<li>In the first iteration flag every vertex as activated and write it again.</li>\n<li>If a vertex is activated, loop through the pointsTo tree and write a message with the (for this vertex) minimal vertex to every element of the tree.</li>\n<li>Merge messages with the related vertex and if we found a new minimum activate the vertex. If nothing was updated then deactivate it.</li>\n</ol>\n<p>And then repeat from point 3 until no vertex can be updated anymore.<br>\nPart 1 and 3 are inside the Map Task, part 2 and 4 are reduce tasks.<br>\n<a href=\"http://codingwiththomas.blogspot.com/2011/04/controlling-hadoop-job-recursion.html\">Look here how you can implement a job recursion using Apache Hadoop.</a></p>\n<p>So after all iteration is done you’ll have the following output:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\"> 0 | VertexWritable \\[minimalVertexId=0, pointsTo=\\[0\\]\\]  \n 1 | VertexWritable \\[minimalVertexId=1, pointsTo=\\[1, 4, 7\\]\\]  \n 2 | VertexWritable \\[minimalVertexId=2, pointsTo=\\[2, 3, 8\\]\\]  \n 3 | VertexWritable \\[minimalVertexId=2, pointsTo=\\[3, 5\\]\\]  \n 4 | VertexWritable \\[minimalVertexId=1, pointsTo=\\[1, 4\\]\\]  \n 5 | VertexWritable \\[minimalVertexId=2, pointsTo=\\[5, 6\\]\\]  \n 6 | VertexWritable \\[minimalVertexId=2, pointsTo=\\[6\\]\\]  \n 7 | VertexWritable \\[minimalVertexId=1, pointsTo=\\[7\\]\\]  \n 8 | VertexWritable \\[minimalVertexId=2, pointsTo=\\[3, 8\\]\\]  \n 9 | VertexWritable \\[minimalVertexId=0, pointsTo=\\[0, 9\\]\\]  \n</code></pre></div>\n<p>So you see that we always have every vertex on the left side, but now the minimalVertexId is the classifier for the component. And we have the three lowest component identifiers found: 0,1 and 2!</p>\n<p>So this looks like that now:</p>\n<p><a href=\"http://4.bp.blogspot.com/-ceRwD7v693Q/TZ9LR45uwPI/AAAAAAAAAQQ/mzk6osE6Xak/s1600/Unbenannt_end.png\"><img src=\"http://4.bp.blogspot.com/-ceRwD7v693Q/TZ9LR45uwPI/AAAAAAAAAQQ/mzk6osE6Xak/s320/Unbenannt_end.png\" alt=\"classified graph with multiple components\"></a></p>\n<p>If you are now interested in getting all vertices to a component identifier you’ll be able to write a new mapper that will extract the minimalVertexId as a key and the pointsTo elements as a value. So that in the reduce step they’ll be merged together and you can persist your data.</p>\n<p>And if you are interested in more source code you are free to look into my Summer of Code project under: <a href=\"http://code.google.com/p/hama-shortest-paths/\">http://code.google.com/p/hama-shortest-paths/</a><br>\nYou’ll find a working algorithm inside of the package called “de.jungblut.mapreduce.graph”. The main function to call is inside the class “DatasetImporter.java”.<br>\nThe example input used in this post is also in the trunk. So check this out and you are welcome to use it for your own problems ;)</p>\n<p>So the next time I write a BSP that will do the same.</p>","frontmatter":{"title":"Graph Exploration with Apache Hadoop and MapReduce","date":"8th April 2011","description":null},"tableOfContents":"","timeToRead":4},"previous":{"fields":{"slug":"/blogger/back-to-blogsphere/"},"frontmatter":{"title":"Back to Blogsphere and how BSP works"}},"next":{"fields":{"slug":"/blogger/controlling-hadoop-job-recursion/"},"frontmatter":{"title":"Controlling Hadoop MapReduce Job recursion"}}},"pageContext":{"id":"b0f28283-b8f1-5d28-976f-653b973f4bb4","previousPostId":"b7ca2c0c-eaba-5b81-96f0-f7a4a3574bdf","nextPostId":"fef60276-987b-5ff2-bb6d-053747a34caf"}},"staticQueryHashes":["2270107033","2841359383"],"slicesMap":{}}
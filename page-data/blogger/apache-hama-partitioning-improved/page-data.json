{"componentChunkName":"component---src-templates-blog-post-js","path":"/blogger/apache-hama-partitioning-improved/","result":{"data":{"site":{"siteMetadata":{"title":"Coding with Thomas"}},"markdownRemark":{"id":"50665042-9183-58b2-838c-152c69195999","excerpt":"Hey guys, my work phase is over and I’m back to university next week so I’m having a bit more time to write here. The first free time yesterday focussed on…","html":"<p>Hey guys,</p>\n<p>my work phase is over and I’m back to university next week so I’m having a bit more time to write here.</p>\n<p>The first free time yesterday focussed on generating random data for Apache Hama / Apache Whirr and for my pagerank algorithm. <a href=\"https://issues.apache.org/jira/browse/HAMA-420\">HAMA-420</a> is the related issue on Jira.</p>\n<p>After a lot of stress with the broken DMOZ file, I came along that partitioning takes far to long for some (1-3) million vertices. I already knew this issue, because it always took nearly 10 minutes to partition the sequencefile (1,26GB) of my SSSP(Single Source Shortest Paths) algorithm.</p>\n<p><strong>Where were the problems?</strong></p>\n<p>Actually, there were two major problems I saw.</p>\n<ul>\n<li> Structure of the in- and output File</li>\n<li> Zlib Compression</li>\n</ul>\n<p>So first off the structure, for SSSP it looked like:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">K           /                V   \nVertex[Text] / AdjacentVertex : Weight [Text]  </code></pre></div>\n<p>Every line contained one(!) vertex and it’s adjacent. Someone of you might notice that this is quite verbose.\nWe used this structure for both, the input and the partitioned files.</p>\n<p>The second thing is the standard compression that comes with SequenceFiles, it is the zLib codec which you have to explicitly turn off. Otherwise compression takes nearly 80% of the writing time without any real effort in file sizes.</p>\n<p><strong>How to deal with it?</strong></p>\n<p>These two problems were quite easy to solve. The first thing is to change structure and put adjacent vertices along with their origin vertex into one line of input textfile.</p>\n<p>To be quite general for inputs I sticked with textfiles (this is very common and readable in every editor) and directly write into a sequencefile which is key/value composal of a vertex and an array of vertices.</p>\n<p><strong>Processing</strong></p>\n<p>The new architecture works on the input textfile and reads each line and passes it into an abstract method.<br>\nSo everyone can implement their own input-textfile-parser and put it into the vertex the BSP needs. Which then gets directly partitioned via hash and gets written into the sequencefile of the groom.</p>\n<p>Later on we have to extend this with various stuff of compression, inputformat, recordreader and other partitioning ways.</p>\n<p>The groom will then just read the sequencefile objects and will put them into ram. So no more text deserialization has to be done: CHEERS!</p>\n<p><strong>Comparision new vs old</strong></p>\n<p>The result is overwhelming! See it in my Calc Sheet:</p>\n<p><a href=\"http://3.bp.blogspot.com/-Js2lDIfGysk/TotPhmgqJuI/AAAAAAAAAVk/RHx7ZlftE6Q/s1600/improvement.PNG\"><img src=\"http://3.bp.blogspot.com/-Js2lDIfGysk/TotPhmgqJuI/AAAAAAAAAVk/RHx7ZlftE6Q/s1600/improvement.PNG\" alt=\"\"></a></p>\n<p>This is just a great improvement. It get’s comitted as <a href=\"https://issues.apache.org/jira/browse/HAMA-423\">HAMA-423</a>.</p>\n<p>Thanks and Bye!</p>","frontmatter":{"title":"Apache Hama Partitioning Improved","date":"20th August 2011","description":null},"tableOfContents":"","timeToRead":2},"previous":{"fields":{"slug":"/blogger/dealing-with-outofmemoryerror-in-hadoop/"},"frontmatter":{"title":"Dealing with \"OutOfMemoryError\" in Hadoop"}},"next":{"fields":{"slug":"/blogger/ant-colony-optimization-for-tsp/"},"frontmatter":{"title":"Ant Colony Optimization for TSP Problems"}}},"pageContext":{"id":"50665042-9183-58b2-838c-152c69195999","previousPostId":"cf020bc8-3503-5f52-9d26-89e834dbf96e","nextPostId":"76b5ebc4-91c3-5d4f-957c-9fd72740c5d6"}},"staticQueryHashes":["2270107033","2841359383"],"slicesMap":{}}
{"componentChunkName":"component---src-templates-blog-post-js","path":"/blogger/named-entity-recognition-in-news/","result":{"data":{"site":{"siteMetadata":{"title":"Coding with Thomas"}},"markdownRemark":{"id":"219995eb-2c17-5ba4-9d68-9b4980627485","excerpt":"Hello, been a while since the last post. Was into a lot of work- couldn’t really get up on the weekend to write about   named entity recognition in news…","html":"<p>Hello,</p>\n<p>been a while since the last post. Was into a lot of work- couldn’t really get up on the weekend to write about   named entity recognition in news articles. But today we can finally talk about it.</p>\n<p>This post is about a few things:</p>\n<ol>\n<li>What is named entity recognition?</li>\n<li>How do we model it as a machine learning problem?</li>\n<li>What features to extract for our learner?</li>\n</ol>\n<p>So let’s dive in, if you have taken the Natural Language Processing (NLP) class on Coursera, you will be familiar with the topic already and should start with the features we use in the third paragraph.</p>\n<p><strong>What is named entity recognition (NER)?</strong></p>\n<p>The most easiest explanation is to find word level concepts like a location or person in an unstructured text file. Let’s say we have the following snippet of text, shamelessly stolen from Wikipedia:</p>\n<blockquote>\n<p><em>Jim bought 300 shares of Acme Corp. in 2006.</em></p>\n</blockquote>\n<p>The idea is to tag parts of this sentence with tuples of concepts and their value, such that we get this:</p>\n<blockquote>\n<p><em><strong>&#x3C;PERSON, “Jim”></strong> bought 300 shares of <strong>&#x3C;CORP,“Acme Corp”></strong> in 2006.</em></p>\n</blockquote>\n<p>So we detected <em>Jim</em> as a person and the <em>Acme Corp</em> as a corporation in this sentence.<br>\nBut how do we need this for our news aggregation engine? A very simple assumption: News are about people, what they did and where they did it. A simple example would be:</p>\n<blockquote>\n<p><em>“David Cameron talks in Davos about the EU”</em></p>\n</blockquote>\n<p>The topic is clearly about the person <em>David Cameron</em>- the action of talking and a location namely <em>Davos</em> in Switzerland. This is needed for our news aggregation engine to cluster topics together, even if their content is slightly different. We will talk about this in one of the following blog posts.</p>\n<p><strong>How do we model this as a machine learning problem?</strong></p>\n<p>Basically, it is nothing else than a (multiclass) classification problem. Namely classifying if a token belongs to the class PERSON, LOCATION- or O which is none of the ones before.</p>\n<p>The main difference to other NLP tasks is that we need the context of a token, because the meaning of the current token is depending on the previous or the following token/class. So let’s have a look at another example:</p>\n<blockquote>\n<p><em>It did not, and most of Mr. Cameron’s European partners do not wish to revisit that fundamental question.</em></p>\n</blockquote>\n<p>How do we recognize Cameron in this case here? There are two cases that are representative for english. First the “Mr.” is a strong indicator that there is following a name, second the “‘s” is a strong indicator that the previous token was a name. Also prepositions like “of”, “to”, “in” or “for” are likely indicators for names or locations. The trick that I learned from the NLP class on coursera was the encoding of the text as a sequence of unigrams. The previous text would look like this:</p>\n<blockquote>\n<p>most O<br>\nof O<br>\nMr. O<br>\nCameron PERSON<br>\n’s O</p>\n</blockquote>\n<p>So what we do is predicting the label of the current unigram, by looking at the previous and following unigram and maybe also at the previous label. The reason we need to look at the previous label is that names could be composed of name and surname like <em>David Cameron</em>. So if the last class was a person, in many cases the current class might also be a person.</p>\n<p>So what kind of classifier do we use? I used a self-written version of the <strong>Maximum Entropy Markov Model</strong> supplied in week four of the NLP class exercise optimizable with normal Gradient Descent or Conjugate Gradient (or even with the given quasi newton minimizer supplied in the class). Also I written some utilities to extract sparse feature vectors as conveniently as in NLP class.</p>\n<p><a href=\"https://github.com/thomasjungblut/thomasjungblut-common/tree/master/src/main/java/de/jungblut/ner\">You can browse some more code in my common repository’s NER package</a>.</p>\n<p><strong>What features to extract for our learner?</strong></p>\n<p>Features are pretty important, they must cover structural as well as dictionary features. Here is my feature set for dictionary features:</p>\n<ul>\n<li>current word</li>\n<li>last word</li>\n<li>next word</li>\n</ul>\n<p>And for structural features:</p>\n<ul>\n<li>previous class label</li>\n<li>current word upper case start character</li>\n<li>last word upper case start character</li>\n<li>current word length</li>\n<li>current word containing only alphabetic characters  (1=true or 0=false)</li>\n<li>next word containing only alphabetic characters  (1=true or 0=false)</li>\n<li>was the last word a preposition</li>\n<li>was the last word a special character like dot, comma or question mark</li>\n<li>last word ending with “ing”</li>\n<li>previous/current/next words POS tag</li>\n</ul>\n<p>POS tags are pretty important as nouns are more likely to be a person or location. Also other POS tags might lead to one of those classes by the previous word or next word, e.G. verbs are pretty likely to follow a name. All these features are pretty sparse, thus we are building a dictionary of all possible features we observed during training time and encode a dictionary of features we have seen.<br>\nFor example the feature <em>prevPosTag</em> could have value <em>“prevPosTag=NN”</em>. This is a rare occation as we have a unigram encoded as a feature vector, so it totally makes sense to encode them with a sparse vector.</p>\n<p>Now that we have our text encoded as a list of vectors (a sparse vector for each unigram we observe) we can optimize the vectors and their outcome by minimizing a conditional likelyhood costfunction to be used in the Markov Model. This will learn conditional probabilities between features and the outcomes, describing when we are observing a feature- how likely is that the PERSON class occurs, for math lovers this can be described as P(class | features). I optimized my model for 1k iterations using conjugate gradient and obtained a very low training error of arround 5%. To obtain the class for a feature vector we are doing a viterbi decoding on the learned weights. The trick here is that you need to encode the feature vector for all the possible classes, only that way the viterbi can decode the probabilities correctly.</p>\n<p>So yeah, that is basically what it’s all that named entity recognition is about. The next blog post will most probably be about how to cluster the news together using the information we gathered through this post.</p>\n<p>Bye!</p>","frontmatter":{"title":"Named Entity Recognition in News Articles","date":"27th January 2013","description":null},"tableOfContents":"","timeToRead":4},"previous":{"fields":{"slug":"/blogger/extracting-articles-from-crawled-html/"},"frontmatter":{"title":"Extracting articles from crawled HTML sites"}},"next":{"fields":{"slug":"/blogger/wiring-services-together-using-actor/"},"frontmatter":{"title":"Wiring services together using the actor model"}}},"pageContext":{"id":"219995eb-2c17-5ba4-9d68-9b4980627485","previousPostId":"509aa5be-1842-5c1b-acc7-5bfc8f240a3f","nextPostId":"48213e99-873d-5b52-8ef1-89c762d76547"}},"staticQueryHashes":["2270107033","2841359383"],"slicesMap":{}}
{"componentChunkName":"component---src-templates-blog-post-js","path":"/blogger/pagerank-with-apache-hama/","result":{"data":{"site":{"siteMetadata":{"title":"Coding with Thomas"}},"markdownRemark":{"id":"7ef4a2b7-40cb-5314-8c74-1d74ecf7a864","excerpt":"Hey, some days ago I read in the Hama-dev mailing list about the Nutch project that want a PageRank implementation: Hi all, Anyone interested in PageRank and…","html":"<p>Hey,</p>\n<p>some days ago I read in the Hama-dev mailing list about the Nutch project that want a PageRank implementation:</p>\n<blockquote>\n<p>Hi all,<br>\nAnyone interested in PageRank and collaborating w/ Nutch project? :-)</p>\n</blockquote>\n<p><a href=\"http://www.mail-archive.com/hama-dev@incubator.apache.org/msg03849.html\">Source</a></p>\n<p>So I thought, that I can do this. I already implemented PageRank with MapReduce. Why don’t we go for a BSP version?:D<br>\nThis is basically what this blog post is about.</p>\n<p>Let’s make a few assumptions:</p>\n<ul>\n<li>We have an adjacency list (web-graphs are sparse) of webpages and their unweighted edges</li>\n<li>A working partitioning <a href=\"http://codingwiththomas.blogspot.com/2011/04/apache-hama-partitioning.html\">like here</a>. (You must not implement it, but should know how it works)</li>\n<li>We have read the Pregel Paper (<a href=\"http://blog.udanax.org/2010/06/summary-of-google-pregel.html\">or at least the summary</a>)</li>\n<li>Familiarity with <a href=\"http://en.wikipedia.org/wiki/PageRank\">PageRank</a></li>\n</ul>\n<p><strong>Web Graph Layout</strong></p>\n<p>This is the adjacency list. On the leftmost side is the vertexID of the webpage, followed by the outlinks that are seperated by tabs.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">1 2 3  \n2  \n3 1 2 5  \n4 5 6  \n5 4 6  \n6 4  \n7 2 4  \n</code></pre></div>\n<p>This will be pretty printed a graph like this:</p>\n<p><a href=\"http://2.bp.blogspot.com/-5O4suZbk0lU/TamvQMd-zJI/AAAAAAAAAQU/bs5a-G5yAIs/s1600/Unbenannt.PNG\"><img src=\"http://2.bp.blogspot.com/-5O4suZbk0lU/TamvQMd-zJI/AAAAAAAAAQU/bs5a-G5yAIs/s320/Unbenannt.PNG\" alt=\"\"></a></p>\n<p>I have colored them by the incoming links, the vertex with the most in-links is the brightest, the vertex with few or no in-links is more darker.<br>\nWe will see that vertex 2 should get a high PageRank, 4, 5 and 6 should get a more or less equal rank and so on.</p>\n<p><strong>Short summary of the algorithm</strong></p>\n<p>I am now referring to the Google Pregel paper. At first we need a modelling class that will represent a vertex and holds its own tentative PageRank. In my case we are storing the tentative PageRank along with the id of a vertex in a HashMap.<br>\nIn the first superstep we set the tentative PageRank to 1 / n. Where n is the number of vertices in the whole graph.<br>\nIn each of the steps we are going to send for every vertex its PageRank, devided by the number of its outgoing edges, to all adjacent vertices in the graph.<br>\nSo from the second step we are receiving messages with a tentative PageRank of a vertex that is adjacent. Now we are summing up these messages for each vertex “i” and using this formula:</p>\n<blockquote>\n<p>P(i) = 0.15/NumVertices() + 0.85 * sum</p>\n</blockquote>\n<p>This is the new tentative PageRank for a vertex “i”.<br>\nI’m not sure whether NumVertices() returns the number of all vertices or just the number of adjacent vertices. I’ll assume that this is the count of all vertices in the graph, this would then be a constant. Now adding the damping factor multiplying this by the sum of the received tentatives of the adjacent vertices.</p>\n<p>We are looping these steps until convergence to a given error will be archived. This error is just a sum of absoluting the difference between the old tentative PageRank and the new one of each vertex.<br>\nOr we can break if we are reaching a iteration that is high enough.</p>\n<p>We are storing the old PageRank as a copy of the current PageRank (simple HashMap).<br>\nThe error will thus be a local variable that we are going to sync with a master task- he will average them and broadcasts it back to all the slaves.</p>\n<p><strong>Code</strong></p>\n<p>Let’s look at the fields we need:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">private static int MAX\\_ITERATIONS = 30;  \n // alpha is 0.15/NumVertices()  \n private static double ALPHA;  \n private static int numOfVertices;  \n private static double DAMPING\\_FACTOR = 0.85;  \n // this is the error we want to archieve  \n private static double EPSILON = 0.001;  \n  \n        HashMap&lt;Integer, List&lt;Integer>> adjacencyList = new HashMap&lt;Integer, List&lt;Integer>>();  \n // normally this is stored by a vertex, but I don't want to create a new  \n // model for it  \n HashMap&lt;Integer, Double> tentativePagerank = new HashMap&lt;Integer, Double>();  \n // backup of the last pagerank to determine the error  \n HashMap&lt;Integer, Double> lastTentativePagerank = new HashMap&lt;Integer, Double>();  \n</code></pre></div>\n<p>Keep in mind that every task just has a subgraph of the graph. So these structures will hold just a chunk of PageRank.</p>\n<p>Let’s get into the init phase of the BSP:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">@Override  \n public void bsp(BSPPeerProtocol peer) throws IOException, KeeperException,  \n   InterruptedException {  \n  fs = FileSystem.get(getConf());  \n  String master = conf.get(MASTER\\_TASK);  \n  // setup the datasets  \n  adjacencyList = mapAdjacencyList(getConf(), peer);  \n  // init the pageranks to 1/n where n is the number of all vertices  \n  for (int vertexId : adjacencyList.keySet())  \n   tentativePagerank  \n     .put(vertexId, Double.valueOf(1.0 / numOfVertices));  \n  \n...  \n</code></pre></div>\n<p>Like we said, we are reading the data chunk from HDFS and going to set the tentative pagerank to 1/n.</p>\n<p><strong>Main Loop</strong></p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">// while the error not converges against epsilon do the pagerank stuff  \n  double error = 1.0;  \n  int iteration = 0;  \n  // if MAX\\_ITERATIONS are set to 0, ignore the iterations and just go  \n  // with the error  \n  while ((MAX\\_ITERATIONS > 0 &amp;&amp; iteration &lt; MAX\\_ITERATIONS)  \n    || error >= EPSILON) {  \n  \n   peer.sync();  \n  \n   if (iteration >= 1) {  \n    // copy the old pagerank to the backup  \n    copyTentativePageRankToBackup();  \n    // sum up all incoming messages for a vertex  \n    HashMap&lt;Integer, Double> sumMap = new HashMap&lt;Integer, Double>();  \n    IntegerDoubleMessage msg = null;  \n    while ((msg = (IntegerDoubleMessage) peer.getCurrentMessage()) != null) {  \n     if (!sumMap.containsKey(msg.getTag())) {  \n      sumMap.put(msg.getTag(), msg.getData());  \n     } else {  \n      sumMap.put(msg.getTag(),  \n        msg.getData() + sumMap.get(msg.getTag()));  \n     }  \n    }  \n    // pregel formula:  \n    // ALPHA = 0.15 / NumVertices()  \n    // P(i) = ALPHA + 0.85 \\* sum  \n    for (Entry&lt;Integer, Double> entry : sumMap.entrySet()) {  \n     tentativePagerank.put(entry.getKey(),  \n       ALPHA + (entry.getValue() \\* DAMPING\\_FACTOR));  \n    }  \n  \n    // determine the error and send this to the master  \n    double err = determineError();  \n    error = broadcastError(peer, master, err);  \n   }  \n   // in every step send the tentative pagerank of a vertex to its  \n   // adjacent vertices  \n   for (int vertexId : adjacencyList.keySet())  \n    sendMessageToNeighbors(peer, vertexId);  \n  \n   iteration++;  \n  }  \n</code></pre></div>\n<p>I guess this is self explaining. The function broadcastError() will send the determined error to a master task, he will average all incoming errors and broadcasts this back to the slaves (similar to aggregators in the Pregel paper).<br>\nLet’s take a quick look at the determineError() function:</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">private double determineError() {  \n  double error = 0.0;  \n  for (Entry&lt;Integer, Double> entry : tentativePagerank.entrySet()) {  \n   error += Math.abs(lastTentativePagerank.get(entry.getKey())  \n     - entry.getValue());  \n  }  \n  return error;  \n }  \n</code></pre></div>\n<p>Like I described in the summary we are just summing up the errors that is a difference between the old and the new rank.</p>\n<p><strong>Output</strong></p>\n<p>Finally we are able to run this and receive a fraction between 0 and 1 that will represent the PageRank of each site.<br>\nI was running this with a convergence error of 0.000001 and a damping factor of 0.85. This took about 17 iterations.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">\\------------------- RESULTS --------------------  \n2 | 0.33983048615390526  \n4 | 0.21342628110369394  \n6 | 0.20495452025114747  \n5 | 0.1268811487940641  \n3 | 0.0425036157080356  \n1 | 0.0425036157080356  \n7 | 0.02990033228111791  \n</code></pre></div>\n<p>This will result in about 1.0 in the sum of all ranks, which is correct.<br>\nNote that the output if you are running this job is not guaranteed to be sorted, I did this to give you a better view.</p>\n<p>We’ll see that we were quite good in our guessing of the PageRank in the beginning.</p>\n<p>I think this is all, if you are interested in testing / running this- feel free to do so.<br>\nThis class and test data is located in my Summer of Code repository under: <a href=\"http://code.google.com/p/hama-shortest-paths/\">http://code.google.com/p/hama-shortest-paths/</a><br>\nThe classes name is <em>de.jungblut.hama.bsp.PageRank</em>.<br>\nJust execute the main method, it will run a local multithreaded BSP on your machine.</p>\n<p>Star this project and vote for my <a href=\"https://issues.apache.org/jira/browse/HAMA-359\">GSoC task</a>. :)</p>\n<p>Thank you.</p>","frontmatter":{"title":"PageRank with Apache Hama","date":"16th April 2011","description":null},"tableOfContents":"","timeToRead":5},"previous":{"fields":{"slug":"/blogger/apache-hama-partitioning/"},"frontmatter":{"title":"Apache Hama Partitioning"}},"next":{"fields":{"slug":"/blogger/apache-hama-network-throughput/"},"frontmatter":{"title":"Apache Hama network throughput"}}},"pageContext":{"id":"7ef4a2b7-40cb-5314-8c74-1d74ecf7a864","previousPostId":"ccc3d38b-5ce4-5d38-afa8-67432bd75a51","nextPostId":"aba86902-77ac-5770-b07f-6a3d23711170"}},"staticQueryHashes":["2270107033","2841359383"],"slicesMap":{}}